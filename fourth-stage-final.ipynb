{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":11476758,"datasetId":7191603,"databundleVersionId":11921330}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nfrom glob import glob\nimport sys\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom scipy.stats import entropy\nfrom scipy.signal import butter, lfilter, freqz\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom tqdm.auto import tqdm\nfrom functools import partial\nimport cv2\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR, CosineAnnealingLR, CosineAnnealingWarmRestarts\nfrom sklearn.preprocessing import LabelEncoder\nfrom torchvision.transforms import v2\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import train_test_split\nimport albumentations as A\nfrom albumentations import (Compose, Normalize, Resize, RandomResizedCrop, HorizontalFlip, VerticalFlip, ShiftScaleRotate, Transpose)\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\nimport timm\nfrom scipy import optimize\nfrom scipy.optimize import minimize_scalar\nimport warnings \nwarnings.filterwarnings('ignore')\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom matplotlib import pyplot as plt\nimport joblib\nVERSION=16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:10:46.722362Z","iopub.execute_input":"2025-04-19T20:10:46.722598Z","iopub.status.idle":"2025-04-19T20:11:03.204101Z","shell.execute_reply.started":"2025-04-19T20:10:46.722580Z","shell.execute_reply":"2025-04-19T20:11:03.203441Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class CFG:\n    # define kaggle input paths \n    data_path = \"/kaggle/input/flood-prediction/\"\n    original_data_path = \"/kaggle/input/flood-prediction/data/\"\n\n    # Files containing OOF predictions from Stage 3 models\n    fastai_oof_file = data_path + \"fastai_train_with_oof.csv\"\n    gbdt_oof_file = data_path + \"gbdt_train_with_oof.csv\"\n\n    # Files containing Test predictions from Stage 3 models\n    fastai_test_file = data_path + \"fastai_test_with_oof.csv\"\n    gbdt_test_file = data_path + \"gbdt_test_with_oof.csv\"\n\n    # Files containing Static Flood Probability from Stage 2\n    stage2_train_file = data_path + \"train_with_cv_results_accuracy_0_8931.csv\"\n    stage2_test_file = data_path + \"test_with_cv_results_accuracy_0_8931.csv\"\n\n    # Sample submission file\n    submission_file = original_data_path + \"SampleSubmission.csv\" # Adjust path if needed\n\n    seed = 2024\n    n_ensemble_models = 3 # We have FastAI, LGB, CATT predictions available\n\ndef seed_everything(seed=CFG.seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    # Torch seeds are not needed here unless using PyTorch directly\n    # torch.manual_seed(seed)\n    # torch.cuda.manual_seed(seed)\n    # torch.backends.cudnn.deterministic = True\n    # torch.backends.cudnn.benchmark = False\n\nseed_everything()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:12:35.382681Z","iopub.execute_input":"2025-04-19T20:12:35.383234Z","iopub.status.idle":"2025-04-19T20:12:35.388266Z","shell.execute_reply.started":"2025-04-19T20:12:35.383208Z","shell.execute_reply":"2025-04-19T20:12:35.387683Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nprint(\"Loading OOF prediction files...\")\n# Load FastAI OOF\nfastai_oof = pd.read_csv(CFG.fastai_oof_file)\nprint(f\"FastAI OOF columns: {fastai_oof.columns.tolist()}\")\n# Load GBDT OOF (contains LGB and CATT)\ngbdt_oof = pd.read_csv(CFG.gbdt_oof_file)\nprint(f\"GBDT OOF columns: {gbdt_oof.columns.tolist()}\")\n\nprint(\"\\nLoading Test prediction files...\")\n# Load FastAI Test preds\nfastai_test = pd.read_csv(CFG.fastai_test_file)\nprint(f\"FastAI Test columns: {fastai_test.columns.tolist()}\")\n# Load GBDT Test preds\ngbdt_test = pd.read_csv(CFG.gbdt_test_file)\nprint(f\"GBDT Test columns: {gbdt_test.columns.tolist()}\")\n\nprint(\"\\nLoading Stage 2 static probability results...\")\n# Load Static Probabilities (OOF for Train)\nstage2_train = pd.read_csv(CFG.stage2_train_file)\nprint(f\"Stage 2 Train columns: {stage2_train.columns.tolist()}\")\n# Load Static Probabilities (Test)\nstage2_test = pd.read_csv(CFG.stage2_test_file)\nprint(f\"Stage 2 Test columns: {stage2_test.columns.tolist()}\")\n\nprint(\"\\nLoading Sample Submission...\")\nsubmission_df = pd.read_csv(CFG.submission_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:12:35.795570Z","iopub.execute_input":"2025-04-19T20:12:35.796237Z","iopub.status.idle":"2025-04-19T20:12:37.590151Z","shell.execute_reply.started":"2025-04-19T20:12:35.796212Z","shell.execute_reply":"2025-04-19T20:12:37.589495Z"}},"outputs":[{"name":"stdout","text":"Loading OOF prediction files...\nFastAI OOF columns: ['event_id', 'location_id', 'event_t', 'flood_probability', 'label', 'oof_fastai']\nGBDT OOF columns: ['event_id', 'location_id', 'event_t', 'flood_probability', 'label', 'oof_lgb', 'oof_catt', 'ensemble_preds']\n\nLoading Test prediction files...\nFastAI Test columns: ['event_id', 'location_id', 'event_t', 'flood_probability', 'label', 'fastai_preds']\nGBDT Test columns: ['event_id', 'location_id', 'event_t', 'flood_probability', 'label', 'lgb_preds', 'catt_preds', 'ensemble_preds']\n\nLoading Stage 2 static probability results...\nStage 2 Train columns: ['location_id', 'label', 'fold', 'image_path', 'oof_preds', 'oof_correct_prob', 'predicted_class']\nStage 2 Test columns: ['location_id', 'event_id_counts', 'image_path', 'predicted_prob', 'predicted_class']\n\nLoading Sample Submission...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- Prepare OOF Data ---\n# Select necessary columns and rename for clarity\noof_df = fastai_oof[['event_id', 'location_id', 'event_t', 'label', 'oof_fastai']].copy()\noof_df = pd.merge(oof_df, gbdt_oof[['event_id', 'oof_lgb', 'oof_catt']], on='event_id', how='left')\n\n# Merge static probability from Stage 2 OOF results\nstage2_train_prob = stage2_train[['location_id', 'oof_correct_prob']].rename(columns={'oof_correct_prob': 'static_prob'})\noof_df = pd.merge(oof_df, stage2_train_prob, on='location_id', how='left')\n\n# Sort to ensure alignment\noof_df = oof_df.sort_values(by=['location_id', 'event_t']).reset_index(drop=True)\n\nprint(\"Prepared OOF DataFrame head:\")\ndisplay(oof_df.head())\nprint(f\"OOF DataFrame shape: {oof_df.shape}\")\nprint(f\"OOF NaNs check:\\n{oof_df.isnull().sum()}\")\n\n\n# --- Prepare Test Data ---\n# Select necessary columns and rename\ntest_df = fastai_test[['event_id', 'location_id', 'event_t', 'fastai_preds']].copy()\ntest_df = pd.merge(test_df, gbdt_test[['event_id', 'lgb_preds', 'catt_preds']], on='event_id', how='left')\n\n# Merge static probability from Stage 2 test results\n# Note: Column name in stage2_test is 'predicted_prob' based on user input\nstage2_test_prob = stage2_test[['location_id', 'predicted_prob']].rename(columns={'predicted_prob': 'static_prob'})\ntest_df = pd.merge(test_df, stage2_test_prob, on='location_id', how='left')\n\n# Sort to ensure alignment (important for final submission)\ntest_df = test_df.sort_values(by='event_id').reset_index(drop=True)\n\nprint(\"\\nPrepared Test DataFrame head:\")\ndisplay(test_df.head())\nprint(f\"Test DataFrame shape: {test_df.shape}\")\nprint(f\"Test NaNs check:\\n{test_df.isnull().sum()}\")\n\n# Handle potential NaNs if merges failed (e.g., fill with mean/median or investigate)\n# For simplicity, let's check and fill static_prob if needed\nmedian_static_prob_oof = oof_df['static_prob'].median()\nmedian_static_prob_test = test_df['static_prob'].median()\noof_df['static_prob'] = oof_df['static_prob'].fillna(median_static_prob_oof)\ntest_df['static_prob'] = test_df['static_prob'].fillna(median_static_prob_test)\nprint(\"\\nNaN check after fill:\")\nprint(f\"OOF NaNs: {oof_df['static_prob'].isnull().sum()}\")\nprint(f\"Test NaNs: {test_df['static_prob'].isnull().sum()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:13:02.205682Z","iopub.execute_input":"2025-04-19T20:13:02.205990Z","iopub.status.idle":"2025-04-19T20:13:03.157192Z","shell.execute_reply.started":"2025-04-19T20:13:02.205970Z","shell.execute_reply":"2025-04-19T20:13:03.156443Z"}},"outputs":[{"name":"stdout","text":"Prepared OOF DataFrame head:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"              event_id      location_id  event_t  label  oof_fastai  \\\n0  id_05v6zjuaf300_X_0  id_05v6zjuaf300        0    0.0    0.000132   \n1  id_05v6zjuaf300_X_1  id_05v6zjuaf300        1    0.0    0.000133   \n2  id_05v6zjuaf300_X_2  id_05v6zjuaf300        2    0.0    0.000126   \n3  id_05v6zjuaf300_X_3  id_05v6zjuaf300        3    0.0    0.000117   \n4  id_05v6zjuaf300_X_4  id_05v6zjuaf300        4    0.0    0.000127   \n\n        oof_lgb      oof_catt  static_prob  \n0  1.178418e-07  1.751966e-07       0.9997  \n1  1.635817e-07  8.539053e-07       0.9997  \n2  1.500422e-07  4.935893e-07       0.9997  \n3  1.439613e-07  3.372869e-07       0.9997  \n4  1.144397e-07  3.060345e-07       0.9997  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_id</th>\n      <th>location_id</th>\n      <th>event_t</th>\n      <th>label</th>\n      <th>oof_fastai</th>\n      <th>oof_lgb</th>\n      <th>oof_catt</th>\n      <th>static_prob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_05v6zjuaf300_X_0</td>\n      <td>id_05v6zjuaf300</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.000132</td>\n      <td>1.178418e-07</td>\n      <td>1.751966e-07</td>\n      <td>0.9997</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_05v6zjuaf300_X_1</td>\n      <td>id_05v6zjuaf300</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.000133</td>\n      <td>1.635817e-07</td>\n      <td>8.539053e-07</td>\n      <td>0.9997</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_05v6zjuaf300_X_2</td>\n      <td>id_05v6zjuaf300</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.000126</td>\n      <td>1.500422e-07</td>\n      <td>4.935893e-07</td>\n      <td>0.9997</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_05v6zjuaf300_X_3</td>\n      <td>id_05v6zjuaf300</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.000117</td>\n      <td>1.439613e-07</td>\n      <td>3.372869e-07</td>\n      <td>0.9997</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_05v6zjuaf300_X_4</td>\n      <td>id_05v6zjuaf300</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.000127</td>\n      <td>1.144397e-07</td>\n      <td>3.060345e-07</td>\n      <td>0.9997</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"OOF DataFrame shape: (492020, 8)\nOOF NaNs check:\nevent_id       0\nlocation_id    0\nevent_t        0\nlabel          0\noof_fastai     0\noof_lgb        0\noof_catt       0\nstatic_prob    0\ndtype: int64\n\nPrepared Test DataFrame head:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                event_id      location_id  event_t  fastai_preds  \\\n0    id_066zz28m11mr_X_0  id_066zz28m11mr        0      0.000165   \n1    id_066zz28m11mr_X_1  id_066zz28m11mr        1      0.000167   \n2   id_066zz28m11mr_X_10  id_066zz28m11mr       10      0.000157   \n3  id_066zz28m11mr_X_100  id_066zz28m11mr      100      0.000176   \n4  id_066zz28m11mr_X_101  id_066zz28m11mr      101      0.000178   \n\n      lgb_preds    catt_preds  static_prob  \n0  6.188000e-08  4.520225e-08     0.000834  \n1  6.776189e-08  2.964910e-08     0.000834  \n2  7.236391e-08  3.108074e-08     0.000834  \n3  8.779468e-06  5.077424e-07     0.000834  \n4  4.431223e-06  1.325167e-07     0.000834  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_id</th>\n      <th>location_id</th>\n      <th>event_t</th>\n      <th>fastai_preds</th>\n      <th>lgb_preds</th>\n      <th>catt_preds</th>\n      <th>static_prob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_066zz28m11mr_X_0</td>\n      <td>id_066zz28m11mr</td>\n      <td>0</td>\n      <td>0.000165</td>\n      <td>6.188000e-08</td>\n      <td>4.520225e-08</td>\n      <td>0.000834</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_066zz28m11mr_X_1</td>\n      <td>id_066zz28m11mr</td>\n      <td>1</td>\n      <td>0.000167</td>\n      <td>6.776189e-08</td>\n      <td>2.964910e-08</td>\n      <td>0.000834</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_066zz28m11mr_X_10</td>\n      <td>id_066zz28m11mr</td>\n      <td>10</td>\n      <td>0.000157</td>\n      <td>7.236391e-08</td>\n      <td>3.108074e-08</td>\n      <td>0.000834</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_066zz28m11mr_X_100</td>\n      <td>id_066zz28m11mr</td>\n      <td>100</td>\n      <td>0.000176</td>\n      <td>8.779468e-06</td>\n      <td>5.077424e-07</td>\n      <td>0.000834</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_066zz28m11mr_X_101</td>\n      <td>id_066zz28m11mr</td>\n      <td>101</td>\n      <td>0.000178</td>\n      <td>4.431223e-06</td>\n      <td>1.325167e-07</td>\n      <td>0.000834</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Test DataFrame shape: (163520, 7)\nTest NaNs check:\nevent_id        0\nlocation_id     0\nevent_t         0\nfastai_preds    0\nlgb_preds       0\ncatt_preds      0\nstatic_prob     0\ndtype: int64\n\nNaN check after fill:\nOOF NaNs: 0\nTest NaNs: 0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# Define target and prediction columns\ntarget_col = 'label'\nmodel_oof_cols = ['oof_fastai', 'oof_lgb', 'oof_catt'] \n\noof_preds_arrays = {}\nprint(\"Individual Model OOF Log Loss Scores:\")\nfor col in model_oof_cols:\n    # Clip predictions for log loss stability\n    preds = np.clip(oof_df[col].values, 1e-15, 1 - 1e-15)\n    oof_preds_arrays[col] = preds # Store clipped array\n    score = log_loss(oof_df[target_col].values, preds)\n    print(f\"- {col}: {score:.6f}\")\n\n# Calculate uniform ensemble OOF predictions\nuniform_ensemble_oof = np.mean(list(oof_preds_arrays.values()), axis=0)\nuniform_ensemble_score = log_loss(oof_df[target_col].values, np.clip(uniform_ensemble_oof, 1e-15, 1 - 1e-15))\nprint(f\"\\nUniform Ensemble OOF Log Loss Score: {uniform_ensemble_score:.6f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:13:25.414999Z","iopub.execute_input":"2025-04-19T20:13:25.415663Z","iopub.status.idle":"2025-04-19T20:13:25.987207Z","shell.execute_reply.started":"2025-04-19T20:13:25.415640Z","shell.execute_reply":"2025-04-19T20:13:25.986413Z"}},"outputs":[{"name":"stdout","text":"Individual Model OOF Log Loss Scores:\n- oof_fastai: 0.002653\n- oof_lgb: 0.002428\n- oof_catt: 0.002672\n\nUniform Ensemble OOF Log Loss Score: 0.002307\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\n# Use a threshold (e.g., 0.7 as used in their final test norm, or optimize it later)\nnormalization_threshold_demo = 0.7\nepsilon = 1e-8\n\nprint(f\"\\nLogloss before normalizing (Uniform Ensemble): {uniform_ensemble_score:.6f}\")\n\n# Create temporary df for calculation\ntemp_oof_df = oof_df[['location_id', 'static_prob', target_col]].copy()\ntemp_oof_df['uniform_ensemble'] = uniform_ensemble_oof\n\nlocations_to_normalize = temp_oof_df[temp_oof_df['static_prob'] >= normalization_threshold_demo]['location_id'].unique()\ntemp_oof_df['oof_sum_prob'] = temp_oof_df.groupby('location_id')['uniform_ensemble'].transform('sum')\n\ntemp_oof_df['ensemble_oof_norm'] = temp_oof_df['uniform_ensemble'] # Copy original values\nmask = temp_oof_df['location_id'].isin(locations_to_normalize)\nvalid_divisor_mask = mask & (temp_oof_df['oof_sum_prob'] > epsilon)\n\ntemp_oof_df.loc[valid_divisor_mask, 'ensemble_oof_norm'] = (\n    temp_oof_df.loc[valid_divisor_mask, 'uniform_ensemble'] /\n    temp_oof_df.loc[valid_divisor_mask, 'oof_sum_prob']\n)\n\n# Clip for log loss\ntemp_oof_df['ensemble_oof_norm'] = np.clip(temp_oof_df['ensemble_oof_norm'], epsilon, 1 - epsilon)\n\nnormalized_uniform_score = log_loss(temp_oof_df[target_col].values, temp_oof_df['ensemble_oof_norm'].values)\nprint(f\"Logloss after normalizing (Uniform Ensemble, Threshold={normalization_threshold_demo}): {normalized_uniform_score:.6f}\")\n\ndel temp_oof_df # Clean up\ngc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:13:25.988580Z","iopub.execute_input":"2025-04-19T20:13:25.989086Z","iopub.status.idle":"2025-04-19T20:13:26.519652Z","shell.execute_reply.started":"2025-04-19T20:13:25.989067Z","shell.execute_reply":"2025-04-19T20:13:26.518931Z"}},"outputs":[{"name":"stdout","text":"\nLogloss before normalizing (Uniform Ensemble): 0.002307\nLogloss after normalizing (Uniform Ensemble, Threshold=0.7): 0.002252\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"21"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"\n# Prepare list of OOF prediction arrays (use the clipped ones)\nens = [oof_preds_arrays[col] for col in model_oof_cols]\nlabels = oof_df[target_col].values\nn_models = len(ens)\n\n# Objective function for the optimizer\ndef objective(weights, predictions, true_labels):\n    \"\"\"Calculates log loss for weighted ensemble predictions.\"\"\"\n    if np.sum(weights) == 0: # Avoid division by zero if all weights are zero\n         return 1.0 # Return high loss\n\n    # Normalize weights to sum to 1 (common practice with Nelder-Mead)\n    weights = weights / np.sum(weights)\n\n    # Calculate weighted prediction\n    ensemble_preds = np.zeros_like(true_labels, dtype=float)\n    for i, w in enumerate(weights):\n        ensemble_preds += w * predictions[i]\n\n    # Clip for stability and calculate log loss\n    ensemble_preds = np.clip(ensemble_preds, 1e-15, 1 - 1e-15)\n    loss = log_loss(true_labels, ensemble_preds)\n    return loss\n\n# Initial guess: equal weights\ninitial_weights = np.array([1.0 / n_models] * n_models)\n\n# Bounds for weights (e.g., >0)\nbounds = [(0.0, 1.0)] * n_models # Weights between 0 and 1\n\nprint(f\"\\nOptimizing weights for {n_models} models...\")\n\n# Use minimize function\n# Nelder-Mead doesn't strictly enforce bounds or constraints during optimization,\n# but the internal normalization in `objective` helps keep weights positive and summing to 1 conceptually.\n# Methods like 'L-BFGS-B' or 'SLSQP' handle bounds/constraints more directly if needed.\nresult = optimize.minimize(\n    objective,\n    initial_weights,\n    args=(ens, labels), # Pass predictions and labels to the objective function\n    method='Nelder-Mead',\n    # bounds=bounds, # Nelder-Mead doesn't use bounds argument directly\n    options={'disp': True, 'maxiter': 1000, 'adaptive': True} # Increase maxiter, use adaptive parameters\n)\n\n# Optimized weights (renormalize just in case)\noptimized_weights = result.x / np.sum(result.x)\nbest_oof_score_optimized = result.fun\n\nprint(f\"\\nOptimization successful: {result.success}\")\nprint(f\"Optimized Weights: {optimized_weights}\")\nprint(f\"Sum of Weights: {np.sum(optimized_weights)}\")\nprint(f\"Optimized Ensemble OOF Log Loss: {best_oof_score_optimized:.6f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:13:26.520548Z","iopub.execute_input":"2025-04-19T20:13:26.520835Z","iopub.status.idle":"2025-04-19T20:13:41.029917Z","shell.execute_reply.started":"2025-04-19T20:13:26.520812Z","shell.execute_reply":"2025-04-19T20:13:41.029032Z"}},"outputs":[{"name":"stdout","text":"\nOptimizing weights for 3 models...\nOptimization terminated successfully.\n         Current function value: 0.002301\n         Iterations: 57\n         Function evaluations: 105\n\nOptimization successful: True\nOptimized Weights: [0.35456422 0.43411358 0.21132219]\nSum of Weights: 0.9999999999999999\nOptimized Ensemble OOF Log Loss: 0.002301\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\n# Select test prediction columns corresponding to OOF columns used in 'ens'\nmodel_test_cols = ['fastai_preds', 'lgb_preds', 'catt_preds'] # Ensure order matches model_oof_cols\ntest_preds_arrays = [test_df[col].values for col in model_test_cols]\n\n# Calculate weighted ensemble predictions for the test set\nweighted_test_preds = np.zeros_like(test_preds_arrays[0], dtype=float)\nfor i, w in enumerate(optimized_weights):\n    weighted_test_preds += w * test_preds_arrays[i]\n\n# Store weighted predictions in the test dataframe\ntest_df['ensemble_label'] = weighted_test_preds\n\nprint(\"Weighted ensemble predictions calculated for test set.\")\ndisplay(test_df[['event_id', 'ensemble_label']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:13:41.030909Z","iopub.execute_input":"2025-04-19T20:13:41.031194Z","iopub.status.idle":"2025-04-19T20:13:41.048286Z","shell.execute_reply.started":"2025-04-19T20:13:41.031173Z","shell.execute_reply":"2025-04-19T20:13:41.047472Z"}},"outputs":[{"name":"stdout","text":"Weighted ensemble predictions calculated for test set.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                event_id  ensemble_label\n0    id_066zz28m11mr_X_0        0.000059\n1    id_066zz28m11mr_X_1        0.000059\n2   id_066zz28m11mr_X_10        0.000056\n3  id_066zz28m11mr_X_100        0.000066\n4  id_066zz28m11mr_X_101        0.000065","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_id</th>\n      <th>ensemble_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_066zz28m11mr_X_0</td>\n      <td>0.000059</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_066zz28m11mr_X_1</td>\n      <td>0.000059</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_066zz28m11mr_X_10</td>\n      <td>0.000056</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_066zz28m11mr_X_100</td>\n      <td>0.000066</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_066zz28m11mr_X_101</td>\n      <td>0.000065</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"\n# --- Optimize Normalization Threshold (Optional but Recommended) ---\n# We use the OOF data to find the best threshold\n\n# Recalculate the OPTIMIZED OOF ensemble prediction\noptimized_oof_preds = np.zeros_like(labels, dtype=float)\nfor i, w in enumerate(optimized_weights):\n    optimized_oof_preds += w * ens[i] # Use original ens OOF arrays\n\ntemp_opt_oof_df = oof_df[['location_id', 'static_prob', target_col]].copy()\ntemp_opt_oof_df['opt_ensemble'] = optimized_oof_preds\n\n# Define the scoring function again for the optimized OOF preds\ndef calculate_normalized_logloss_opt(threshold, df):\n    locations_to_normalize = df[df['static_prob'] >= threshold]['location_id'].unique()\n    df['oof_sum_prob'] = df.groupby('location_id')['opt_ensemble'].transform('sum')\n    epsilon = 1e-8\n    df['preds_norm'] = df['opt_ensemble'] # Default to original optimized ensemble\n\n    mask = df['location_id'].isin(locations_to_normalize)\n    valid_divisor_mask = mask & (df['oof_sum_prob'] > epsilon)\n\n    df.loc[valid_divisor_mask, 'preds_norm'] = (\n        df.loc[valid_divisor_mask, 'opt_ensemble'] /\n        df.loc[valid_divisor_mask, 'oof_sum_prob']\n    )\n    df['preds_norm'] = np.clip(df['preds_norm'], epsilon, 1 - epsilon)\n    return log_loss(df['label'], df['preds_norm'])\n\nprint(\"\\nOptimizing normalization threshold on OPTIMIZED OOF ensemble...\")\nopt_thresh_result = minimize_scalar(\n    lambda t: calculate_normalized_logloss_opt(t, temp_opt_oof_df.copy()),\n    bounds=(0.01, 0.99),\n    method='bounded'\n)\n\nbest_threshold_final = opt_thresh_result.x\nbest_final_oof_score_normalized = opt_thresh_result.fun\n\nprint(f\"Optimal Normalization Threshold (from optimized OOF): {best_threshold_final:.4f}\")\nprint(f\"Best OOF Log Loss (Optimized Ensemble + Optimized Norm): {best_final_oof_score_normalized:.6f}\")\n\ndel temp_opt_oof_df # Clean up\ngc.collect()\n\n\n# --- Apply Normalization to Test Predictions using the optimized threshold ---\nprint(f\"\\nApplying normalization to TEST predictions using threshold: {best_threshold_final:.4f}\")\n\nlocations_to_normalize_test = test_df[test_df['static_prob'] >= best_threshold_final]['location_id'].unique()\ntest_df['oof_sum_prob'] = test_df.groupby('location_id')['ensemble_label'].transform('sum')\n\ntest_df['final_label'] = test_df['ensemble_label'] # Copy weighted predictions\n\nmask_test = test_df['location_id'].isin(locations_to_normalize_test)\nvalid_divisor_mask_test = mask_test & (test_df['oof_sum_prob'] > epsilon)\n\ntest_df.loc[valid_divisor_mask_test, 'final_label'] = (\n    test_df.loc[valid_divisor_mask_test, 'ensemble_label'] /\n    test_df.loc[valid_divisor_mask_test, 'oof_sum_prob']\n)\n\n# Final clip for submission\ntest_df['final_label'] = np.clip(test_df['final_label'], epsilon, 1 - epsilon)\n\nprint(\"Normalization applied to test predictions.\")\ndisplay(test_df[['event_id', 'ensemble_label', 'final_label']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:13:41.050621Z","iopub.execute_input":"2025-04-19T20:13:41.050965Z","iopub.status.idle":"2025-04-19T20:13:44.691653Z","shell.execute_reply.started":"2025-04-19T20:13:41.050935Z","shell.execute_reply":"2025-04-19T20:13:44.691069Z"}},"outputs":[{"name":"stdout","text":"\nOptimizing normalization threshold on OPTIMIZED OOF ensemble...\nOptimal Normalization Threshold (from optimized OOF): 0.6863\nBest OOF Log Loss (Optimized Ensemble + Optimized Norm): 0.002241\n\nApplying normalization to TEST predictions using threshold: 0.6863\nNormalization applied to test predictions.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                event_id  ensemble_label  final_label\n0    id_066zz28m11mr_X_0        0.000059     0.000059\n1    id_066zz28m11mr_X_1        0.000059     0.000059\n2   id_066zz28m11mr_X_10        0.000056     0.000056\n3  id_066zz28m11mr_X_100        0.000066     0.000066\n4  id_066zz28m11mr_X_101        0.000065     0.000065","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_id</th>\n      <th>ensemble_label</th>\n      <th>final_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_066zz28m11mr_X_0</td>\n      <td>0.000059</td>\n      <td>0.000059</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_066zz28m11mr_X_1</td>\n      <td>0.000059</td>\n      <td>0.000059</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_066zz28m11mr_X_10</td>\n      <td>0.000056</td>\n      <td>0.000056</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_066zz28m11mr_X_100</td>\n      <td>0.000066</td>\n      <td>0.000066</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_066zz28m11mr_X_101</td>\n      <td>0.000065</td>\n      <td>0.000065</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"\n# Ensure submission is sorted by event_id, matching test_df\nsubmission_df = submission_df.sort_values('event_id').reset_index(drop=True)\n\n# Create submission file using the FINAL normalized predictions\nfinal_submission = test_df[['event_id', 'final_label']].rename(columns={'final_label': 'label'})\n\n# Verify alignment before assigning\nif not submission_df['event_id'].equals(final_submission['event_id']):\n     print(\"Error: event_id mismatch between sample submission and predictions!\")\n     # Attempt merge as fallback\n     submission_final = pd.merge(submission_df[['event_id']], final_submission, on='event_id', how='left')\nelse:\n     submission_final = final_submission\n\nprint(\"\\nFinal Submission DataFrame head:\")\ndisplay(submission_final.head())\n\n# Save to /kaggle/working/\noutput_filename = \"/kaggle/working/submission_adapted_ensemble.csv\"\nsubmission_final.to_csv(output_filename, index=False)\n\nprint(f\"\\nSubmission file saved to {output_filename}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:13:44.692272Z","iopub.execute_input":"2025-04-19T20:13:44.692450Z","iopub.status.idle":"2025-04-19T20:13:45.253764Z","shell.execute_reply.started":"2025-04-19T20:13:44.692436Z","shell.execute_reply":"2025-04-19T20:13:45.253135Z"}},"outputs":[{"name":"stdout","text":"\nFinal Submission DataFrame head:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                event_id     label\n0    id_066zz28m11mr_X_0  0.000059\n1    id_066zz28m11mr_X_1  0.000059\n2   id_066zz28m11mr_X_10  0.000056\n3  id_066zz28m11mr_X_100  0.000066\n4  id_066zz28m11mr_X_101  0.000065","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_066zz28m11mr_X_0</td>\n      <td>0.000059</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_066zz28m11mr_X_1</td>\n      <td>0.000059</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_066zz28m11mr_X_10</td>\n      <td>0.000056</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_066zz28m11mr_X_100</td>\n      <td>0.000066</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_066zz28m11mr_X_101</td>\n      <td>0.000065</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nSubmission file saved to /kaggle/working/submission_adapted_ensemble.csv\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# End of assignment","metadata":{}}]}