{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models # Keep for potential future use\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR # Import scheduler\n",
    "\n",
    "import collections\n",
    "from collections.abc import Callable, Sequence\n",
    "from concurrent import futures\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "from typing import Any, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import skimage as ski\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd898e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/zindi_data/\"\n",
    "path = \"/kaggle/input/inundata-mapping-floods-in-south-africa/\"\n",
    "train = pd.read_csv(path + \"Train.csv\")\n",
    "test = pd.read_csv(path + \"Test.csv\")\n",
    "submission = pd.read_csv(path + \"SampleSubmission.csv\")\n",
    "images = np.load(path + \"composite_images.npz\")\n",
    "display(train.head(), test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90918c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(value):\n",
    "  return value.split(\"_\")[0] + '_' + value.split(\"_\")[1]\n",
    "\n",
    "def get_event_id(value):\n",
    "  return value.split(\"_\")[3]\n",
    "for df in [train, test]:\n",
    "  df['location_id'] = df['event_id'].apply(lambda x: get_location(x))\n",
    "  df['event'] = df['event_id'].apply(lambda x: get_event_id(x))\n",
    "\n",
    "print(len(set(train['location_id'])), len(set(test['location_id'])))\n",
    "print(len(set(train['location_id']).intersection(set(test['location_id']))))\n",
    "print(len(images))\n",
    "display(train.head(), test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(['location_id'])['event_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the training into train and validation group \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_grouped, test_size=0.2, random_state=42, stratify=train_grouped['label']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870775b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_SIZE = (224, 224)  # Standard input size for many CNNs\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGE_TYPES = [\"moisture-stress\", \"nir11slope\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d28a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FloodEventDataset(Dataset):\n",
    "    def __init__(self, root_dir, labels_df, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.labels_df = labels_df.set_index('location_id')['label'].to_dict()\n",
    "        self.labels_df = {str(k): v for k, v in self.labels_df.items()}\n",
    "        # print(\"THESE ARE THE LABELS\", labels_df)\n",
    "        self.transform = transform\n",
    "\n",
    "        # The subfolders: type1_folder to type5_folder\n",
    "        self.type_folders = [\"moisture-stress\", \"nir11slope\"]\n",
    "\n",
    "        # Get filenames from the first folder\n",
    "        folder_path = os.path.join(root_dir, self.type_folders[0])\n",
    "        # print(\"THIS IS FOLDER_PATH\", folder_path)\n",
    "        all_files = os.listdir(folder_path)\n",
    "\n",
    "        self.event_filenames = [\n",
    "            f for f in all_files if f.endswith('.png') and f[:-4] in self.labels_df\n",
    "        ]\n",
    "        # print(\"THIS IS EVENT_FILENAMES\", self.event_filenames)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.event_filenames[idx]\n",
    "        images = []\n",
    "\n",
    "        for folder in self.type_folders:\n",
    "            img_path = os.path.join(self.root_dir, folder, filename)\n",
    "            img = Image.open(img_path)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "\n",
    "        # Shape: (5, C, H, W)\n",
    "        stacked_images = torch.stack(images)\n",
    "        location_id = filename.split(\".\")[0]\n",
    "        label = torch.tensor(self.labels_df[location_id], dtype=torch.float32)\n",
    "        return stacked_images, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the training into train and validation group \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_grouped, test_size=0.2, random_state=42, stratify=train_grouped['label']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299be373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Assume `labels_df` has columns: location_id, flood_label\n",
    "train_dataset = FloodEventDataset(root_dir='/kaggle/input', labels_df=train_df, transform=transform)\n",
    "print(len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990080d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = FloodEventDataset(root_dir='/kaggle/input', labels_df=val_df, transform=transform)\n",
    "print(len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders \n",
    "imgs, label = validation_dataset[0]\n",
    "\n",
    "print(imgs.shape)  # torch.Size([2, 3, 224, 224])\n",
    "print(label)       # tensor(0.) or tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab5926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image classification model\n",
    "\n",
    "class FloodClassifier(nn.Module):\n",
    "    def __init__(self, num_image_types=5, pretrained=True):\n",
    "        super(FloodClassifier, self).__init__()\n",
    "        \n",
    "        # Feature extractor (using ResNet18)\n",
    "        self.feature_extractors = nn.ModuleList([\n",
    "            self._create_feature_extractor(pretrained) for _ in range(num_image_types)\n",
    "        ])\n",
    "        \n",
    "        # Calculate feature size (ResNet18 produces 512 features)\n",
    "        feature_size = 512 * num_image_types\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def _create_feature_extractor(self, pretrained):\n",
    "        # Using ResNet18 as feature extractor\n",
    "        model = models.resnet18(pretrained=pretrained)\n",
    "        # Remove final classification layer\n",
    "        modules = list(model.children())[:-1]\n",
    "        feature_extractor = nn.Sequential(*modules)\n",
    "        \n",
    "        # Freeze the feature extractor\n",
    "        for param in feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        return feature_extractor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_image_types, channels, height, width]\n",
    "        batch_size, num_imgs, c, h, w = x.shape\n",
    "        \n",
    "        # Process each image type\n",
    "        features = []\n",
    "        for i in range(num_imgs):\n",
    "            # Extract the i-th image type from all batches\n",
    "            img_i = x[:, i, :, :, :]  # [batch_size, channels, height, width]\n",
    "            # Get features\n",
    "            feat_i = self.feature_extractors[i](img_i)  # [batch_size, feature_size, 1, 1]\n",
    "            feat_i = feat_i.view(batch_size, -1)  # [batch_size, feature_size]\n",
    "            features.append(feat_i)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined_features = torch.cat(features, dim=1)  # [batch_size, feature_size * num_image_types]\n",
    "        \n",
    "        # Apply classifier\n",
    "        output = self.classifier(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
    "    # Track best model\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    \n",
    "    # For tracking metrics\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # Move to device\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).view(-1, 1)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE).view(-1, 1)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = val_correct / val_total\n",
    "        \n",
    "        # Save metrics\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        history['val_acc'].append(val_epoch_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "              f'Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f} - '\n",
    "              f'Val Loss: {val_epoch_loss:.4f} - Val Acc: {val_epoch_acc:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_epoch_loss < best_val_loss:\n",
    "            best_val_loss = val_epoch_loss\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flood_classifier(train_dataset, val_dataset):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    val_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    # Initialize model\n",
    "    model = FloodClassifier(num_image_types=len(IMAGE_TYPES), pretrained=True)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Set up loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, num_epochs=EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Save model\n",
    "    # torch.save(model.state_dict(), \"flood_classifier_pytorch.pth\")\n",
    "    torch.save(model.state_dict(), '/kaggle/working/flood_classifier_pytorch.pth')\n",
    "    print(\"Model saved as 'flood_classifier_pytorch.pth'\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfeb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_flood_classifier(train_dataset, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87145c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_csv(model, train_dataset, train_df, validation_dataset, val_df, csv_output_path=\"flood_predictions.csv\", random_state=42):\n",
    "  \n",
    "    \n",
    "    # Create transform for inference\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    val_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    \n",
    "    # For train set\n",
    "    train_probs = []\n",
    "    train_true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            probs = outputs.cpu().numpy().flatten()\n",
    "            train_probs.extend(probs)\n",
    "            train_true_labels.extend(labels.numpy())\n",
    "\n",
    "    predicted_train_df = pd.DataFrame({\n",
    "        'location_id': train_df['location_id'],\n",
    "        'flood_probability': train_probs,\n",
    "        'predicted_label': (np.array(train_probs) >= 0.5).astype(int),\n",
    "        'true_label': train_true_labels,\n",
    "        'dataset': 'train'\n",
    "    })\n",
    "        \n",
    "        \n",
    "    \n",
    "    # For validation set\n",
    "    val_probs = []\n",
    "    val_true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            probs = outputs.cpu().numpy().flatten()\n",
    "            val_probs.extend(probs)\n",
    "            val_true_labels.extend(labels.numpy())\n",
    "\n",
    "    predicted_val_df = pd.DataFrame({\n",
    "        'location_id': val_df['location_id'],\n",
    "        'flood_probability': val_probs,\n",
    "        'predicted_label': (np.array(val_probs) >= 0.5).astype(int),\n",
    "        'true_label': val_true_labels,\n",
    "        'dataset': 'validation'\n",
    "    })\n",
    "    \n",
    "    # Create DataFrames for train and validation sets\n",
    "    \n",
    "    \n",
    "    # # Combine and save to CSV\n",
    "    combined_df = pd.concat([predicted_train_df, predicted_val_df])\n",
    "    combined_df.to_csv(csv_output_path, index=False)\n",
    "    \n",
    "    # print(f\"Predictions saved to {csv_output_path}\")\n",
    "    \n",
    "    # Optional: Calculate and display some metrics\n",
    "    train_accuracy = np.mean((np.array(train_probs) >= 0.5).astype(int) == np.array(train_true_labels))\n",
    "    val_accuracy = np.mean((np.array(val_probs) >= 0.5).astype(int) == np.array(val_true_labels))\n",
    "    \n",
    "    print(f\"Train accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = generate_prediction_csv(model, train_dataset, train_df, validation_dataset, val_df, \"/kaggle/working/flood_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
